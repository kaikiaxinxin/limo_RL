import rospy
import torch
import numpy as np
import os
import params
from agent import TD3_Dual_Critic
# å‡è®¾æ‚¨çš„å®è½¦ç¯å¢ƒæ–‡ä»¶åå« stl_real_env_pro.py
from stl_real_env_pro import STL_Real_Env 

# === Sim-to-Real åæ ‡å¯¹é½é…ç½® ===
# å®è½¦å¼€æœºæ—¶çš„åŸç‚¹ (0,0) å¯¹åº”ä»¿çœŸä¸–ç•Œä¸­çš„å“ªä¸ªåæ ‡ï¼Ÿ
# ä»¿çœŸä¸­ Robot å‡ºç”Ÿåœ¨ (-7.0, 0.0)ï¼Œæ‰€ä»¥åç§»é‡ä¸º X=-7.0, Y=0.0
INITIAL_OFFSET_X = -7.0
INITIAL_OFFSET_Y = 0.0

def main():
    rospy.init_node('stl_td3_deploy')
    
    # 1. ç¯å¢ƒåˆå§‹åŒ–
    try:
        env = STL_Real_Env()
        print("âœ… Environment initialized.")
    except Exception as e:
        print(f"âŒ Env Error: {e}")
        return

    # 2. Agent åˆå§‹åŒ–
    # ç¡®ä¿ params.STATE_DIM ä¸è®­ç»ƒæ—¶ä¸€è‡´
    agent = TD3_Dual_Critic()
    
    # 3. æ¨¡å‹åŠ è½½
    # è¯·ä¿®æ”¹ä¸ºæ‚¨æ•ˆæœæœ€å¥½çš„æ¨¡å‹åç§° (ä¸è¦åŠ åç¼€)
    model_name = "best_model_5000" 
    model_path = os.path.join(params.MODEL_DIR, model_name)
    
    print(f"ğŸ”„ Loading model from: {model_path} ...")
    if not os.path.exists(model_path + "_actor"):
        print(f"âŒ Model file not found: {model_path}_actor")
        return
        
    agent.load(model_path)
    print("âœ… Model loaded successfully.")

    # 4. å®‰å…¨å¯åŠ¨ç¡®è®¤
    print("\n" + "="*40)
    print("âš ï¸  WARNING: Robot is about to move!")
    print(f"   - Alignment Offset: X={INITIAL_OFFSET_X}, Y={INITIAL_OFFSET_Y}")
    print("   - Please ensure the robot is facing the correct direction (Sim X+).")
    input("ğŸ‘‰ Press Enter to START autonomous navigation...")
    print("="*40 + "\n")

    # 5. ä¸»å¾ªç¯
    # [å…³é”®] é¢‘ç‡å¿…é¡»ä¸è®­ç»ƒæ—¶çš„ (1 / (DT * ACTION_REPEAT)) ä¸€è‡´
    # å‡è®¾ params.DT=0.1, ACTION_REPEAT=2 -> 5Hz
    rate = rospy.Rate(5) 
    
    try:
        while not rospy.is_shutdown():
            # === [æ ¸å¿ƒä¿®æ­£] åæ ‡ç³»æ³¨å…¥ ===
            # è·å–å®è½¦åŸå§‹é‡Œç¨‹è®¡æ•°æ® (ç›¸å¯¹äºå¼€æœºç‚¹)
            raw_x, raw_y = env.pose_odom[0], env.pose_odom[1]
            
            # åŠ ä¸Šåç§»é‡ï¼Œè½¬æ¢ä¸ºä»¿çœŸä¸–ç•Œåæ ‡
            # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ä¿®æ”¹ env å†…éƒ¨å˜é‡ï¼Œä»¥ä¾¿ _get_obs() è®¡ç®—ç›¸å¯¹ç›®æ ‡è·ç¦»æ—¶ä½¿ç”¨æ­£ç¡®çš„ä¸–ç•Œåæ ‡
            env.pose_odom[0] = raw_x + INITIAL_OFFSET_X
            env.pose_odom[1] = raw_y + INITIAL_OFFSET_Y
            
            # è·å–çŠ¶æ€ (ç½‘ç»œè¾“å…¥)
            # æ­¤æ—¶ _get_obs å†…éƒ¨è®¡ç®—çš„ distance å·²ç»æ˜¯åŸºäºä¸–ç•Œåæ ‡çš„äº†
            state = env._get_obs()
            
            # --- è°ƒè¯•æ‰“å° ---
            # æ‰“å°çœ‹ä¸€ä¸‹è½¬æ¢åçš„åæ ‡æ˜¯å¦ç¬¦åˆé¢„æœŸ (åº”è¯¥æ¥è¿‘ -7.0, 0.0)
            # print(f"Odom Raw: ({raw_x:.2f}, {raw_y:.2f}) -> World: ({env.pose_odom[0]:.2f}, {env.pose_odom[1]:.2f})")
            
            # æ¨ç†åŠ¨ä½œ
            action = agent.select_action(state)
            
            # [å®‰å…¨] å®è½¦é€Ÿåº¦å†æ¬¡æˆªæ–­ (åŒé‡ä¿é™©)
            # å³ä½¿è®­ç»ƒæ—¶ Max_V æ˜¯ 0.5ï¼Œè¿™é‡Œä¹Ÿå¯ä»¥é™åˆ¶å¾—æ›´æ­»ä¸€ç‚¹
            safe_v = np.clip(action[0], 0.0, 0.4) 
            safe_w = np.clip(action[1], -1.0, 1.0)
            
            # æ‰§è¡ŒåŠ¨ä½œ (ä¼ é€’ç»™ç¯å¢ƒ)
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¼ åŸå§‹ action æˆ– safe_action éƒ½å¯ä»¥ï¼Œå»ºè®®ä¼  safe
            env.step(np.array([safe_v, safe_w]))
            
            # æ‰“å°ä»»åŠ¡è¿›åº¦
            # è®¡ç®—å½“å‰ä½ç½®åˆ°å½“å‰ç›®æ ‡çš„è·ç¦»
            curr_goal = env.get_current_goal_pos()
            dist_to_goal = np.linalg.norm(np.array(env.pose_odom[:2]) - curr_goal)
            
            print(f"Task: {env.current_target_idx} | Dist: {dist_to_goal:.2f}m | V: {safe_v:.2f}, W: {safe_w:.2f}")
            
            rate.sleep()
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ Stopping by user request.")
    except Exception as e:
        print(f"\nâŒ Runtime Error: {e}")
    finally:
        # é€€å‡ºæ—¶å¼ºåˆ¶åœè½¦
        env.stop()
        print("ğŸ‘‹ Robot Stopped.")

if __name__ == '__main__':
    main()